{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18721f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m     64\u001b[0m     [transforms\u001b[38;5;241m.\u001b[39mResize([SIZE,SIZE]),\n\u001b[1;32m     65\u001b[0m      transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m     66\u001b[0m     ])\n\u001b[1;32m     68\u001b[0m celeba_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 69\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCelebA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mceleba_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m trainset \u001b[38;5;241m=\u001b[39m get_balanced_data(trainset)\n\u001b[1;32m     75\u001b[0m trainset, valset \u001b[38;5;241m=\u001b[39m split(trainset, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.8\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/datasets/celeba.py:80\u001b[0m, in \u001b[0;36mCelebA.__init__\u001b[0;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_transform is specified but target_type is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/datasets/celeba.py:150\u001b[0m, in \u001b[0;36mCelebA.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (file_id, md5, filename) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list:\n\u001b[0;32m--> 150\u001b[0m     \u001b[43mdownload_file_from_google_drive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m extract_archive(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_align_celeba.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/datasets/utils.py:259\u001b[0m, in \u001b[0;36mdownload_file_from_google_drive\u001b[0;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[1;32m    256\u001b[0m         api_response, content \u001b[38;5;241m=\u001b[39m _extract_gdrive_api_response(response)\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m api_response \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuota exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    260\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe daily quota of the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is exceeded and it \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be downloaded. This is a limitation of Google Drive \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand can only be overcome by trying again later.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m         )\n\u001b[1;32m    265\u001b[0m     _save_response_content(content, fpath)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# In case we deal with an unhandled GDrive API response, the file should be smaller than 10kB and contain only text\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.linalg import norm\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "SEED = 5636\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "def get_balanced_data(dataset, num_samples=None):\n",
    "\n",
    "    if num_samples is None:\n",
    "        num_samples = len(dataset)\n",
    "\n",
    "    # Make balanced classes\n",
    "    labelset = {}\n",
    "    for i in range(NUM_CLASSES):\n",
    "        one_hot = torch.zeros(NUM_CLASSES)\n",
    "        one_hot[i] = 1\n",
    "        labelset[i] = one_hot\n",
    "\n",
    "    # All attributes found in list_attr_celeba.txt\n",
    "    feature_idx = 31  # Index of feature label - 15 corresponds to glasses, 31 is smiling\n",
    "    by_class = {}\n",
    "    features = []\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        if idx > 10000:\n",
    "            break\n",
    "        ex, label = dataset[idx]\n",
    "        features.append(label[feature_idx])\n",
    "        g = label[feature_idx].numpy().item()\n",
    "        ex = ex.flatten()\n",
    "        ex = ex / norm(ex)\n",
    "        if g in by_class:\n",
    "            by_class[g].append((ex, labelset[g]))\n",
    "        else:\n",
    "            by_class[g] = [(ex, labelset[g])]\n",
    "        if idx > num_samples:\n",
    "            break\n",
    "    data = []\n",
    "    max_len = min(25000, len(by_class[1]))\n",
    "\n",
    "    data.extend(by_class[1][:max_len])\n",
    "    data.extend(by_class[0][:max_len])\n",
    "    return data\n",
    "\n",
    "def split(trainset, p=.8):\n",
    "    train, val = train_test_split(trainset, train_size=p)\n",
    "    return train, val\n",
    "\n",
    "\n",
    "SIZE = 64\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize([SIZE,SIZE]),\n",
    "     transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "celeba_path = './'\n",
    "trainset = torchvision.datasets.CelebA(root=celeba_path,\n",
    "                                       split='train',\n",
    "                                       transform=transform,\n",
    "                                       download=False)\n",
    "\n",
    "trainset = get_balanced_data(trainset)\n",
    "trainset, valset = split(trainset, p=.8)\n",
    "\n",
    "print(\"Train Size: \", len(trainset), \"Val Size: \", len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.CelebA(root=celeba_path,\n",
    "                                      split='test',\n",
    "                                      transform=transform,\n",
    "                                      download=True)\n",
    "\n",
    "testset = get_balanced_data(testset)\n",
    "print(\"Test Size: \", len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=1)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=100,\n",
    "                                        shuffle=False, num_workers=1)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=512,\n",
    "                                         shuffle=False, num_workers=1)\n",
    "\n",
    "def get_data(loader):\n",
    "    X = []\n",
    "    y = []\n",
    "    for idx, batch in enumerate(loader):\n",
    "        inputs, labels = batch\n",
    "        X.append(inputs)\n",
    "        y.append(labels)\n",
    "    return torch.cat(X, dim=0).numpy(), torch.cat(y, dim=0).numpy()\n",
    "\n",
    "X_train, y_train = get_data(train_loader)\n",
    "X_val, y_val = get_data(val_loader)\n",
    "X_test, y_test = get_data(test_loader)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ab501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rfm  \n",
    "from tqdm import tqdm \n",
    "\n",
    "model = rfm.RFM()\n",
    "model = model.fit(X_train[:2000], y_train[:2000], num_iters=1, reg=0, \n",
    "                  centering=True, verbose=True, diag_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "M = model.get_M()\n",
    "diag = np.diag(M).reshape(3, 64, 64)\n",
    "diag = np.rollaxis(diag, 0, 3)\n",
    "diag = (diag - diag.min()) / (diag.max() - diag.min())\n",
    "print(diag.shape)\n",
    "plt.imshow(diag)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ed75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
